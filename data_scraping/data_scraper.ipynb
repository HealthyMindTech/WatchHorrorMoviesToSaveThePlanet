{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: BeautifulSoup4 in ./.venv/lib/python3.10/site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.10/site-packages (from BeautifulSoup4) (2.3.2.post1)\n"
     ]
    }
   ],
   "source": [
    "!pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to scrape images from a website\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# On the website I want all the images from the page\n",
    "# I want to save them in a folder called \"images\"\n",
    "\n",
    "website = \"https://moviebarcode.tumblr.com/\"\n",
    "page = requests.get(website)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all images and save them to a file with the file name matching their alt text\n",
    "def scrape_images_on_alt_text(soup):\n",
    "    # find all images under div class=\"photo\"\n",
    "    images = soup.find_all(\"img\")\n",
    "    for image in images:\n",
    "        image_url = image['src']\n",
    "        # ensure their is an alt text\n",
    "        if image.has_attr('alt'):\n",
    "            image_name = image['alt']\n",
    "            # ensure the image name is safe\n",
    "            image_name = image_name.replace('/', '')\n",
    "            # do not download the image if it already exists or if their is not alt text\n",
    "            if not os.path.exists(image_name):\n",
    "                # if I get a 404 error print and skip\n",
    "                try:\n",
    "                    urllib.request.urlretrieve(image_url, \"images/\" + image_name + \".jpg\")\n",
    "                except:\n",
    "                    print(f\"404 error\")\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'find'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scrape_images_on_alt_text(soup)\n",
      "Cell \u001b[0;32mIn [25], line 4\u001b[0m, in \u001b[0;36mscrape_images_on_alt_text\u001b[0;34m(soup)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscrape_images_on_alt_text\u001b[39m(soup):\n\u001b[1;32m      3\u001b[0m     \u001b[39m# find all images under div class=\"photo\"\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     images \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39;49mfind_all(\u001b[39m\"\u001b[39;49m\u001b[39mdiv\u001b[39;49m\u001b[39m\"\u001b[39;49m, {\u001b[39m\"\u001b[39;49m\u001b[39mclass\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mphoto\u001b[39;49m\u001b[39m\"\u001b[39;49m})\u001b[39m.\u001b[39;49mfind(\u001b[39m\"\u001b[39m\u001b[39mimg\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images:\n\u001b[1;32m      6\u001b[0m         image_url \u001b[39m=\u001b[39m image[\u001b[39m'\u001b[39m\u001b[39msrc\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/git.repositories/HealthyMindTech/MovieZap/.venv/lib/python3.10/site-packages/bs4/element.py:2289\u001b[0m, in \u001b[0;36mResultSet.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m   2288\u001b[0m     \u001b[39m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2289\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m   2290\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mResultSet object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. You\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m key\n\u001b[1;32m   2291\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: ResultSet object has no attribute 'find'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "scrape_images_on_alt_text(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page https://moviebarcode.tumblr.com/page/217\n"
     ]
    }
   ],
   "source": [
    "# now I want to loop it on all the pages of the website\n",
    "# max number of pages is 217\n",
    "\n",
    "startpage = 1 # for when I get 404\n",
    "maxpages = 217\n",
    "for pagenum in range(startpage, maxpages + 1):\n",
    "    website = f\"https://moviebarcode.tumblr.com/page/{pagenum}\"\n",
    "    print(f\"Scraping page {website}\")\n",
    "    page = requests.get(website)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    scrape_images_on_alt_text(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9120\n",
      "9120\n",
      "5820\n",
      "7200\n",
      "6360\n",
      "8280\n",
      "6840\n",
      "6600\n",
      "7380\n",
      "6000\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# find all images and save them to a file with the file name matching their alt text\n",
    "def get_imdb_links(soup):\n",
    "    # find the object post\n",
    "    posts = soup.find_all(\"div\", class_=\"post\")\n",
    "    for post in posts:\n",
    "        # find the link with www.imdb.com in it\n",
    "        links = post.find_all(\"a\")\n",
    "        for link in links:\n",
    "            if \"www.imdb.com\" in link['href']:\n",
    "                # get the length of the movie\n",
    "                movie_length = get_movie_length(link['href'].replace(\"https://href.li/?\", \"\"))\n",
    "                exit()\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def get_movie_length(link):\n",
    "    movie_page = requests.get(link)\n",
    "    movie_soup = BeautifulSoup(movie_page.content, 'html.parser')\n",
    "\n",
    "    # find the tag script with id =\"__NEXT_DATA__\" and get the json\n",
    "    script = movie_soup.find(\"script\", id=\"__NEXT_DATA__\")\n",
    "    movie_details = json.loads(script.contents[0])\n",
    "    print(movie_details[\"props\"][\"pageProps\"][\"aboveTheFoldData\"][\"runtime\"][\"seconds\"])\n",
    "    # print(movie_details['runtimeConfig'])\n",
    "    # for key in movie_details:\n",
    "    #     print(key)\n",
    "    # print(json.dumps(movie_details, indent=4))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "get_imdb_links(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit ('.venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2335b84ccbd1ea26117a6179f8aae6fea447a2b8e04435e372d2294e0eb7c59f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
